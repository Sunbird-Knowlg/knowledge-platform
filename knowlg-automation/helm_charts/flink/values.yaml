namespace: "flink-dev"
imagepullsecrets: ""
dockerhub: ""
repository: "docker.io/anandp504/sunbird-oneclick"
image_tag: "1.0.0"
serviceMonitor:
  enabled: false
replicaCount: 1

jobmanager:
  rpc_port: 6123
  blob_port: 6124
  query_port: 6125
  ui_port: 8081
  prom_port: 9250
  heap_memory: 1024

rest_port: 80
resttcp_port: 8081
service:
  type: ClusterIP


taskmanager:
  prom_port: 9251
  rpc_port: 6122
  heap_memory: 1024
  replicas: 1
  cpu_requests: 0.3

checkpoint_store_type: ""

# AWS S3 Details
s3_access_key: ""
s3_secret_key: ""
s3_endpoint: ""
s3_path_style_access: ""

# Azure Container Details
azure_account: ""
azure_secret: ""

log4j_console_properties: |
  # This affects logging for both user code and Flink
  rootLogger.level = INFO
  rootLogger.appenderRef.console.ref = ConsoleAppender

  # Uncomment this if you want to _only_ change Flink's logging
  #logger.flink.name = org.apache.flink
  #logger.flink.level = INFO

  # The following lines keep the log level of common libraries/connectors on
  # log level INFO. The root logger does not override this. You have to manually
  # change the log levels here.
  logger.akka.name = akka
  logger.akka.level = ERROR
  logger.kafka.name= org.apache.kafka
  logger.kafka.level = ERROR
  logger.hadoop.name = org.apache.hadoop
  logger.hadoop.level = ERROR
  logger.zookeeper.name = org.apache.zookeeper
  logger.zookeeper.level = ERROR

  # Log all infos to the console
  appender.console.name = ConsoleAppender
  appender.console.type = CONSOLE
  appender.console.layout.type = PatternLayout
  appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n

  # Suppress the irrelevant (wrong) warnings from the Netty channel handler
  logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
  logger.netty.level = OFF

base_config: |
  kafka {
    broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    producer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    consumer.broker-servers = "kafka-headless.kafka.svc.cluster.local:9092"
    zookeeper = "kafka-zookeeper-headless.svc.cluster.local:2181"
    producer {
      max-request-size = 1572864
      batch.size = 98304
      linger.ms = 10
      compression = "snappy"
    }
  }
  job {
    env = "dev"
    enable.distributed.checkpointing = false
    statebackend {
      blob {
        storage {
          account = "obsrvacc.blob.core.windows.net"
          container = "obsrv-blob"
          checkpointing.dir = "checkpoint"
        }
      }
      base.url = ""
    }
  }
  task {
    parallelism = 1
    consumer.parallelism = 1
    checkpointing.compressed = true
    checkpointing.interval = 10
    checkpointing.pause.between.seconds = 3000
    restart-strategy.attempts = 3
    restart-strategy.delay = 30000 # in milli-seconds
  }

  redisdb.connection.timeout = 100
  redis {
    host = localhost
    port = 6379
  }

  redis-meta {
    host = localhost
    port = 6379
  }

  postgres {
    host = localhost
    port = 5432
    maxConnections = 2
    user = "postgres"
    password = "postgres"
  }

  lms-cassandra {
    host = "localhost"
    port = "9042"
  }

druid-validator:
  druid-validator: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = dev.telemetry.denorm
      output.telemetry.route.topic = dev.druid.events.telemetry
      output.summary.route.topic = dev.druid.events.summary
      output.failed.topic = dev.telemetry.failed
      output.duplicate.topic = dev.telemetry.duplicate
      groupId = dev-druid-validator-group
    }
    task {
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
      druid.validation.enabled = true
      druid.deduplication.enabled = false
    }
    schema {
      path {
        telemetry = "schemas/telemetry"
        summary = "schemas/summary"
      }
      file {
        default = envelope.json
        summary = me_workflow_summary.json
        search = search.json
      }
    }
    redis {
      database {
        duplicationstore.id = 8
        key.expiry.seconds = 3600
      }
    }

  flink-conf: |+
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m

job_classname: org.sunbird.dp.validator.task.DruidValidatorStreamTask